diff --git a/GraspPlanner.py b/GraspPlanner.py
index 5e07c6f..f7b7121 100644
--- a/GraspPlanner.py
+++ b/GraspPlanner.py
@@ -17,18 +17,19 @@ def MakeGripperFrames(X_WG, t0=0):
     # Set the timing
     times = {"initial": t0}
     X_GinitialGprepick = X_WG["initial"].inverse() @ X_WG["prepick"]
-    times["prepick"] = times["initial"] + np.linalg.norm(
-        X_GinitialGprepick.translation()
-    )
+    # times["prepick"] = times["initial"] + np.linalg.norm(
+    #     X_GinitialGprepick.translation()
+    # )
+    times["prepick"] = times["initial"] + 0.8
     times["pick_start"] = times["prepick"] + 0.5
     # times["pick_end"] = times["pick_start"] + 1.0
-    times["pick_end"] = 2.5
+    times["pick_end"] = times["pick_start"] + 0.2
     X_WG["pick_start"] = X_WG["pick"]
     X_WG["pick_end"] = X_WG["pick"]
     # times["postpick"] = times["pick_end"] + 1.0
-    times["postpick"] = 3
+    times["postpick"] = times["pick_end"] + 0.4
     X_WG["postpick"] = RigidTransform(
-        RotationMatrix.MakeXRotation(-np.pi / 2), [0, 0, 0.7]
+        RotationMatrix.MakeXRotation(-np.pi / 2), [0, 0, 0.5]
     )
     times["end"] = times["postpick"] + 10.0
     X_WG["end"] = X_WG["postpick"]
diff --git a/GraspSelector.py b/GraspSelector.py
index 37ac9e9..659c04e 100644
--- a/GraspSelector.py
+++ b/GraspSelector.py
@@ -163,15 +163,15 @@ def GenerateAntipodalGraspCandidate(
         wsg = plant.GetBodyByName("body")
         wsg_body_index = wsg.index()
 
-    index = rng.integers(0, cloud.size() - 1)
+    index = rng.integers(0, cloud.size())
 
     # Use S for sample point/frame.
     p_WS = cloud.xyz(index)
     n_WS = cloud.normal(index)
 
-    assert np.isclose(
-        np.linalg.norm(n_WS), 1.0
-    ), f"Normal has magnitude: {np.linalg.norm(n_WS)}"
+    if not np.isclose(np.linalg.norm(n_WS), 1.0):
+        print(f"Normal has magnitude: {np.linalg.norm(n_WS)}")
+        return np.inf, None
 
     Gx = n_WS  # gripper x axis aligns with normal
     # make orthonormal y axis, aligned with world down
diff --git a/__pycache__/GraspPlanner.cpython-310.pyc b/__pycache__/GraspPlanner.cpython-310.pyc
index 880c059..251e61f 100644
Binary files a/__pycache__/GraspPlanner.cpython-310.pyc and b/__pycache__/GraspPlanner.cpython-310.pyc differ
diff --git a/__pycache__/GraspSelector.cpython-310.pyc b/__pycache__/GraspSelector.cpython-310.pyc
index 14d9e43..e9e9f99 100644
Binary files a/__pycache__/GraspSelector.cpython-310.pyc and b/__pycache__/GraspSelector.cpython-310.pyc differ
diff --git a/__pycache__/utils.cpython-310.pyc b/__pycache__/utils.cpython-310.pyc
index 1354541..18069e7 100644
Binary files a/__pycache__/utils.cpython-310.pyc and b/__pycache__/utils.cpython-310.pyc differ
diff --git a/envs/residual_one.py b/envs/residual_one.py
index 9d5e743..b6fb51b 100644
--- a/envs/residual_one.py
+++ b/envs/residual_one.py
@@ -53,6 +53,7 @@ class SE3Adder(LeafSystem):
 
     def CalcOutput(self, context, output):
         r = self.get_input_port(0).Eval(context)
+        # print("Action =", r)
         cost, transform = self.get_input_port(1).Eval(context)
         if not np.isfinite(cost):
             transform = RigidTransform.Identity()
@@ -101,7 +102,7 @@ OBJECTS = {
         "url": "package://manipulation/hydro/010_potted_meat_can.sdf",
     },
 }
-height_threshold = 0.3  # z-coord higher than this threshold is considered as a success
+height_threshold = 0.25  # z-coord higher than this threshold is considered as a success
 
 # Camera parameters
 width = 80
@@ -113,10 +114,10 @@ far = 10.0
 renderer = "my_renderer"
 image_size = width * height * 4
 CAMERA_INSTANCE_PREFIX = "camera"
-cloud_size = 150
+cloud_size = 400
 
 # Gym parameters
-sim_time_step = 0.001
+sim_time_step = 0.005
 gym_time_step = 0.1
 controller_time_step = 0.01
 gym_time_limit = 5
@@ -144,7 +145,7 @@ def reset_all_objects(plant, context=None, active="sugar", rng=None):
             )
             x = rng.random() * 0.1 - 0.05
             y = rng.random() * 0.1 - 0.05
-            z = rng.random() * 0.2 + 0.3
+            z = rng.random() * 0.1 + 0.15
             transform = RigidTransform(RotationMatrix(Quaternion(q)), [x, y, z])
         else:
             transform = RigidTransform([1, 1, -1])
@@ -383,7 +384,7 @@ def make_sim(meshcat=None, time_limit=5, wait_time=0.5, debug=False, obs_noise=F
             object_state = self.get_input_port(0).Eval(context)
             gripper_state = self.get_input_port(1).Eval(context)
             cost = np.linalg.norm(object_state[4:7] - gripper_state[:3]) ** 2
-            reward = 100 if object_state[6] >= 0.3 else 1
+            reward = 100 if object_state[6] >= height_threshold else 1
             if reward > 1:
                 print("Reward =", reward)
             if time > wait_time:
@@ -409,9 +410,6 @@ def make_sim(meshcat=None, time_limit=5, wait_time=0.5, debug=False, obs_noise=F
     def monitor(context):
         scenario_context = scenario.GetMyContextFromRoot(context)
         obj_state = scenario.GetOutputPort("object_state").Eval(scenario_context)
-        # idx = scenario.GetOutputPort("active_obj_index").Eval(scenario_context)
-        # print(f"object #{idx} z-position = {obj_state[6]}")
-
         if obj_state[6] < 0:
             print("Terminal: Object falls below 0.")
             return EventStatus.ReachedTermination(diagram, "object falls below 0")
@@ -483,6 +481,7 @@ class CustomDrakeGymEnv(DrakeGymEnv):
         observation_port_id: str = None,
         reset_handler: Callable[[Simulator, Context], None] = None,
         info_handler: Callable[[Simulator, Context], dict] = None,
+        wait_time: float = 0.5,
     ):
         super().__init__(
             simulator=simulator,
@@ -495,6 +494,7 @@ class CustomDrakeGymEnv(DrakeGymEnv):
             reset_handler=reset_handler,
             info_handler=info_handler,
         )
+        self._wait_time = wait_time
 
     def step(self, action):
         context = self.simulator.get_context()
@@ -504,8 +504,8 @@ class CustomDrakeGymEnv(DrakeGymEnv):
 
         prev_observation = self.observation_port.Eval(context)
         try:
-            if time < 0.5:
-                status = self.simulator.AdvanceTo(0.51)
+            if time < self._wait_time:
+                status = self.simulator.AdvanceTo(self._wait_time)
             else:
                 status = self.simulator.AdvanceTo(3.1)
         except RuntimeError as e:
@@ -531,8 +531,9 @@ class CustomDrakeGymEnv(DrakeGymEnv):
 def DrakeResidualGraspOneStepEnv(
     meshcat=None, time_limit=gym_time_limit, debug=False, obs_noise=False
 ):
+    wait_time = 0.8
     simulator = make_sim(
-        meshcat=meshcat, time_limit=time_limit, debug=debug, obs_noise=obs_noise
+        meshcat=meshcat, time_limit=time_limit, wait_time=wait_time, debug=debug, obs_noise=obs_noise
     )
 
     # Define action space
diff --git a/images/ResidualGraspOne-v0-diagram.png b/images/ResidualGraspOne-v0-diagram.png
index 23c842b..05d2950 100644
Binary files a/images/ResidualGraspOne-v0-diagram.png and b/images/ResidualGraspOne-v0-diagram.png differ
diff --git a/models/floor.sdf b/models/floor.sdf
index bc373e4..34e23c1 100644
--- a/models/floor.sdf
+++ b/models/floor.sdf
@@ -5,7 +5,7 @@
     geometry -->
     <link name="box">
       <inertial>
-        <mass>1000.0</mass>
+        <mass>100.0</mass>
         <inertia>
           <ixx>100</ixx>
           <ixy>0</ixy>
@@ -30,7 +30,7 @@
         <pose>0 0 0 0 0 -0.05</pose>
         <geometry>
           <box>
-            <size>2 2 0.08</size>
+            <size>2 2 0.1</size>
           </box>
         </geometry>
       </collision>
diff --git a/runs/0420/PPO_13/events.out.tfevents.1745168528.mech-d23074.austin.utexas.edu.2236535.0 b/runs/0420/PPO_13/events.out.tfevents.1745168528.mech-d23074.austin.utexas.edu.2236535.0
index c8f7cf1..45490c5 100644
Binary files a/runs/0420/PPO_13/events.out.tfevents.1745168528.mech-d23074.austin.utexas.edu.2236535.0 and b/runs/0420/PPO_13/events.out.tfevents.1745168528.mech-d23074.austin.utexas.edu.2236535.0 differ
diff --git a/wandb/debug-internal.log b/wandb/debug-internal.log
index 585a391..4acb6f2 120000
--- a/wandb/debug-internal.log
+++ b/wandb/debug-internal.log
@@ -1 +1 @@
-run-20250420_120201-8jipcypo/logs/debug-internal.log
\ No newline at end of file
+run-20250420_183149-dfvj23tj/logs/debug-internal.log
\ No newline at end of file
diff --git a/wandb/debug.log b/wandb/debug.log
index b49a224..87416ce 120000
--- a/wandb/debug.log
+++ b/wandb/debug.log
@@ -1 +1 @@
-run-20250420_120201-8jipcypo/logs/debug.log
\ No newline at end of file
+run-20250420_183149-dfvj23tj/logs/debug.log
\ No newline at end of file
diff --git a/wandb/latest-run b/wandb/latest-run
index d94be8f..379b1e8 120000
--- a/wandb/latest-run
+++ b/wandb/latest-run
@@ -1 +1 @@
-run-20250420_120201-8jipcypo
\ No newline at end of file
+run-20250420_183149-dfvj23tj
\ No newline at end of file
diff --git a/wandb/run-20250420_120201-8jipcypo/files/output.log b/wandb/run-20250420_120201-8jipcypo/files/output.log
index 880749a..37dfdf9 100644
--- a/wandb/run-20250420_120201-8jipcypo/files/output.log
+++ b/wandb/run-20250420_120201-8jipcypo/files/output.log
@@ -79,3 +79,183 @@ Logging to runs/0420/PPO_13
 |    std                  | 1.01        |
 |    value_loss           | 35.9        |
 -----------------------------------------
+-----------------------------------------
+| rollout/                |             |
+|    ep_len_mean          | 2.99        |
+|    ep_rew_mean          | 1.92        |
+| time/                   |             |
+|    fps                  | 21          |
+|    iterations           | 5           |
+|    time_elapsed         | 8399        |
+|    total_timesteps      | 184320      |
+| train/                  |             |
+|    approx_kl            | 0.010857548 |
+|    clip_fraction        | 0.131       |
+|    clip_range           | 0.2         |
+|    entropy_loss         | -9.99       |
+|    explained_variance   | 2.86e-05    |
+|    learning_rate        | 0.0003      |
+|    loss                 | 101         |
+|    n_updates            | 20          |
+|    policy_gradient_loss | -0.00528    |
+|    std                  | 1.01        |
+|    value_loss           | 2.04e+03    |
+-----------------------------------------
+-----------------------------------------
+| rollout/                |             |
+|    ep_len_mean          | 2.96        |
+|    ep_rew_mean          | 1.92        |
+| time/                   |             |
+|    fps                  | 22          |
+|    iterations           | 6           |
+|    time_elapsed         | 10049       |
+|    total_timesteps      | 221184      |
+| train/                  |             |
+|    approx_kl            | 0.010978606 |
+|    clip_fraction        | 0.141       |
+|    clip_range           | 0.2         |
+|    entropy_loss         | -9.97       |
+|    explained_variance   | 0.00576     |
+|    learning_rate        | 0.0003      |
+|    loss                 | 0.0066      |
+|    n_updates            | 25          |
+|    policy_gradient_loss | -0.00612    |
+|    std                  | 1           |
+|    value_loss           | 54.7        |
+-----------------------------------------
+-----------------------------------------
+| rollout/                |             |
+|    ep_len_mean          | 2.99        |
+|    ep_rew_mean          | 1.86        |
+| time/                   |             |
+|    fps                  | 22          |
+|    iterations           | 7           |
+|    time_elapsed         | 11726       |
+|    total_timesteps      | 258048      |
+| train/                  |             |
+|    approx_kl            | 0.011744231 |
+|    clip_fraction        | 0.142       |
+|    clip_range           | 0.2         |
+|    entropy_loss         | -9.95       |
+|    explained_variance   | 0.00357     |
+|    learning_rate        | 0.0003      |
+|    loss                 | -0.0151     |
+|    n_updates            | 30          |
+|    policy_gradient_loss | -0.0062     |
+|    std                  | 1           |
+|    value_loss           | 74.6        |
+-----------------------------------------
+----------------------------------------
+| rollout/                |            |
+|    ep_len_mean          | 2.98       |
+|    ep_rew_mean          | 1.87       |
+| time/                   |            |
+|    fps                  | 22         |
+|    iterations           | 8          |
+|    time_elapsed         | 13345      |
+|    total_timesteps      | 294912     |
+| train/                  |            |
+|    approx_kl            | 0.01144138 |
+|    clip_fraction        | 0.141      |
+|    clip_range           | 0.2        |
+|    entropy_loss         | -9.96      |
+|    explained_variance   | -3.81e-06  |
+|    learning_rate        | 0.0003     |
+|    loss                 | 0.00169    |
+|    n_updates            | 35         |
+|    policy_gradient_loss | -0.00494   |
+|    std                  | 1.01       |
+|    value_loss           | 1.27e+06   |
+----------------------------------------
+-----------------------------------------
+| rollout/                |             |
+|    ep_len_mean          | 2.95        |
+|    ep_rew_mean          | 1.27        |
+| time/                   |             |
+|    fps                  | 22          |
+|    iterations           | 9           |
+|    time_elapsed         | 15055       |
+|    total_timesteps      | 331776      |
+| train/                  |             |
+|    approx_kl            | 0.011626828 |
+|    clip_fraction        | 0.142       |
+|    clip_range           | 0.2         |
+|    entropy_loss         | -9.96       |
+|    explained_variance   | 0.00577     |
+|    learning_rate        | 0.0003      |
+|    loss                 | 0.665       |
+|    n_updates            | 40          |
+|    policy_gradient_loss | -0.00535    |
+|    std                  | 1           |
+|    value_loss           | 55.1        |
+-----------------------------------------
+-----------------------------------------
+| rollout/                |             |
+|    ep_len_mean          | 2.96        |
+|    ep_rew_mean          | -1.78       |
+| time/                   |             |
+|    fps                  | 22          |
+|    iterations           | 10          |
+|    time_elapsed         | 16672       |
+|    total_timesteps      | 368640      |
+| train/                  |             |
+|    approx_kl            | 0.011154767 |
+|    clip_fraction        | 0.14        |
+|    clip_range           | 0.2         |
+|    entropy_loss         | -9.97       |
+|    explained_variance   | 0.00209     |
+|    learning_rate        | 0.0003      |
+|    loss                 | 296         |
+|    n_updates            | 45          |
+|    policy_gradient_loss | -0.00506    |
+|    std                  | 1.01        |
+|    value_loss           | 134         |
+-----------------------------------------
+Traceback (most recent call last):
+  File "/home/yyan-admin/yl43338/residual_grasping/train_one_step_e2e.py", line 129, in <module>
+    sys.exit(main())
+  File "/home/yyan-admin/yl43338/residual_grasping/train_one_step_e2e.py", line 116, in main
+    model.learn(
+  File "/home/yyan-admin/yl43338/env/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py", line 311, in learn
+    return super().learn(
+  File "/home/yyan-admin/yl43338/env/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py", line 324, in learn
+    continue_training = self.collect_rollouts(self.env, callback, self.rollout_buffer, n_rollout_steps=self.n_steps)
+  File "/home/yyan-admin/yl43338/env/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py", line 218, in collect_rollouts
+    new_obs, rewards, dones, infos = env.step(clipped_actions)
+  File "/home/yyan-admin/yl43338/env/lib/python3.10/site-packages/stable_baselines3/common/vec_env/base_vec_env.py", line 222, in step
+    return self.step_wait()
+  File "/home/yyan-admin/yl43338/env/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py", line 137, in step_wait
+    results = [remote.recv() for remote in self.remotes]
+  File "/home/yyan-admin/yl43338/env/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py", line 137, in <listcomp>
+    results = [remote.recv() for remote in self.remotes]
+  File "/usr/lib/python3.10/multiprocessing/connection.py", line 250, in recv
+    buf = self._recv_bytes()
+  File "/usr/lib/python3.10/multiprocessing/connection.py", line 414, in _recv_bytes
+    buf = self._recv(4)
+  File "/usr/lib/python3.10/multiprocessing/connection.py", line 379, in _recv
+    chunk = read(handle, remaining)
+KeyboardInterrupt
+Traceback (most recent call last):
+  File "/home/yyan-admin/yl43338/residual_grasping/train_one_step_e2e.py", line 129, in <module>
+    sys.exit(main())
+  File "/home/yyan-admin/yl43338/residual_grasping/train_one_step_e2e.py", line 116, in main
+    model.learn(
+  File "/home/yyan-admin/yl43338/env/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py", line 311, in learn
+    return super().learn(
+  File "/home/yyan-admin/yl43338/env/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py", line 324, in learn
+    continue_training = self.collect_rollouts(self.env, callback, self.rollout_buffer, n_rollout_steps=self.n_steps)
+  File "/home/yyan-admin/yl43338/env/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py", line 218, in collect_rollouts
+    new_obs, rewards, dones, infos = env.step(clipped_actions)
+  File "/home/yyan-admin/yl43338/env/lib/python3.10/site-packages/stable_baselines3/common/vec_env/base_vec_env.py", line 222, in step
+    return self.step_wait()
+  File "/home/yyan-admin/yl43338/env/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py", line 137, in step_wait
+    results = [remote.recv() for remote in self.remotes]
+  File "/home/yyan-admin/yl43338/env/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py", line 137, in <listcomp>
+    results = [remote.recv() for remote in self.remotes]
+  File "/usr/lib/python3.10/multiprocessing/connection.py", line 250, in recv
+    buf = self._recv_bytes()
+  File "/usr/lib/python3.10/multiprocessing/connection.py", line 414, in _recv_bytes
+    buf = self._recv(4)
+  File "/usr/lib/python3.10/multiprocessing/connection.py", line 379, in _recv
+    chunk = read(handle, remaining)
+KeyboardInterrupt
diff --git a/wandb/run-20250420_120201-8jipcypo/logs/debug-internal.log b/wandb/run-20250420_120201-8jipcypo/logs/debug-internal.log
index 93797d0..669435b 100644
--- a/wandb/run-20250420_120201-8jipcypo/logs/debug-internal.log
+++ b/wandb/run-20250420_120201-8jipcypo/logs/debug-internal.log
@@ -6,3 +6,11 @@
 {"time":"2025-04-20T12:02:01.671121657-05:00","level":"INFO","msg":"handler: started","stream_id":"8jipcypo"}
 {"time":"2025-04-20T12:02:01.82193005-05:00","level":"INFO","msg":"Starting system monitor"}
 {"time":"2025-04-20T12:02:18.192629596-05:00","level":"INFO","msg":"tensorboard: no root directory after 10 seconds, using working directory"}
+{"time":"2025-04-20T16:46:13.21465247-05:00","level":"INFO","msg":"stream: closing","id":"8jipcypo"}
+{"time":"2025-04-20T16:46:13.214671999-05:00","level":"INFO","msg":"Stopping system monitor"}
+{"time":"2025-04-20T16:46:13.214711139-05:00","level":"INFO","msg":"Stopped system monitor"}
+{"time":"2025-04-20T16:46:13.557248225-05:00","level":"INFO","msg":"fileTransfer: Close: file transfer manager closed"}
+{"time":"2025-04-20T16:46:13.640940983-05:00","level":"INFO","msg":"handler: closed","stream_id":"8jipcypo"}
+{"time":"2025-04-20T16:46:13.64100428-05:00","level":"INFO","msg":"sender: closed","stream_id":"8jipcypo"}
+{"time":"2025-04-20T16:46:13.640997219-05:00","level":"INFO","msg":"writer: Close: closed","stream_id":"8jipcypo"}
+{"time":"2025-04-20T16:46:13.64118093-05:00","level":"INFO","msg":"stream: closed","id":"8jipcypo"}
diff --git a/wandb/run-20250420_120201-8jipcypo/logs/debug.log b/wandb/run-20250420_120201-8jipcypo/logs/debug.log
index eec8216..4a31af2 100644
--- a/wandb/run-20250420_120201-8jipcypo/logs/debug.log
+++ b/wandb/run-20250420_120201-8jipcypo/logs/debug.log
@@ -22,3 +22,4 @@ config: {'policy_type': 'MlpPolicy', 'total_timesteps': 500000.0, 'env_name': 'O
 2025-04-20 12:02:01,916 INFO    MainThread:2236535 [wandb_init.py:init():1056] run started, returning control to user process
 2025-04-20 12:02:08,191 INFO    MainThread:2236535 [wandb_run.py:_tensorboard_callback():1524] tensorboard callback: runs/0420/PPO_13, True
 2025-04-20 12:02:08,193 INFO    MainThread:2236535 [wandb_run.py:_config_callback():1327] config_cb None None {'algo': 'PPO', 'policy_class': "<class 'stable_baselines3.common.policies.ActorCriticPolicy'>", 'device': 'cpu', 'verbose': 1, 'policy_kwargs': '{}', 'num_timesteps': 0, '_total_timesteps': 500000.0, '_num_timesteps_at_start': 0, 'seed': 'None', 'action_noise': 'None', 'start_time': 1745168526045631733, 'learning_rate': 0.0003, 'tensorboard_log': 'runs/0420', '_last_obs': '[[ 0.          0.          0.         ...  0.          0.\n   0.        ]\n [ 0.03061293  0.03198892  0.01950267 ...  0.          0.\n   0.        ]\n [ 0.          0.          0.         ...  0.          0.\n   0.        ]\n ...\n [-0.00492451 -0.00811863 -0.0100185  ...  0.          0.\n   0.        ]\n [ 0.          0.          0.         ...  0.          0.\n   0.        ]\n [ 0.          0.          0.         ...  0.          0.\n   0.        ]]', '_last_episode_starts': '[ True  True  True  True  True  True  True  True  True  True  True  True\n  True  True  True  True  True  True  True  True  True  True  True  True\n  True  True  True  True  True  True  True  True  True  True  True  True]', '_last_original_obs': 'None', '_episode_num': 0, 'use_sde': 'False', 'sde_sample_freq': -1, '_current_progress_remaining': 1.0, '_stats_window_size': 100, 'ep_info_buffer': 'deque([], maxlen=100)', 'ep_success_buffer': 'deque([], maxlen=100)', '_n_updates': 0, '_custom_logger': 'False', 'env': '<stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x7c89275129b0>', '_vec_normalize_env': 'None', 'observation_space': 'Box(-1.0, 1.0, (3000,), float64)', 'action_space': 'Box([-1.   -1.   -1.   -1.   -0.2  -0.2   0.05], [1.   1.   1.   1.   0.2  0.2  0.25], (7,), float64)', 'n_envs': 36, 'n_steps': 1024, 'gamma': 0.99, 'gae_lambda': 0.95, 'ent_coef': 0.0, 'vf_coef': 0.5, 'max_grad_norm': 0.5, 'rollout_buffer_class': "<class 'stable_baselines3.common.buffers.RolloutBuffer'>", 'rollout_buffer_kwargs': '{}', 'batch_size': 64, 'n_epochs': 5, 'clip_range': '<function get_schedule_fn.<locals>.<lambda> at 0x7c89043c8e50>', 'clip_range_vf': 'None', 'normalize_advantage': 'True', 'target_kl': 'None', 'lr_schedule': '<function get_schedule_fn.<locals>.<lambda> at 0x7c8927555f30>', 'rollout_buffer': '<stable_baselines3.common.buffers.RolloutBuffer object at 0x7c89279c8580>', 'policy': 'ActorCriticPolicy(\n  (features_extractor): FlattenExtractor(\n    (flatten): Flatten(start_dim=1, end_dim=-1)\n  )\n  (pi_features_extractor): FlattenExtractor(\n    (flatten): Flatten(start_dim=1, end_dim=-1)\n  )\n  (vf_features_extractor): FlattenExtractor(\n    (flatten): Flatten(start_dim=1, end_dim=-1)\n  )\n  (mlp_extractor): MlpExtractor(\n    (policy_net): Sequential(\n      (0): Linear(in_features=3000, out_features=64, bias=True)\n      (1): Tanh()\n      (2): Linear(in_features=64, out_features=64, bias=True)\n      (3): Tanh()\n    )\n    (value_net): Sequential(\n      (0): Linear(in_features=3000, out_features=64, bias=True)\n      (1): Tanh()\n      (2): Linear(in_features=64, out_features=64, bias=True)\n      (3): Tanh()\n    )\n  )\n  (action_net): Linear(in_features=64, out_features=7, bias=True)\n  (value_net): Linear(in_features=64, out_features=1, bias=True)\n)', '_logger': '<stable_baselines3.common.logger.Logger object at 0x7c8905752d70>'}
+2025-04-20 16:46:13,213 INFO    MsgRouterThr:2236535 [mailbox.py:close():129] [no run ID] Closing mailbox, abandoning 1 handles.
diff --git a/wandb/run-20250420_120201-8jipcypo/run-8jipcypo.wandb b/wandb/run-20250420_120201-8jipcypo/run-8jipcypo.wandb
index 42e3446..9a90e40 100644
Binary files a/wandb/run-20250420_120201-8jipcypo/run-8jipcypo.wandb and b/wandb/run-20250420_120201-8jipcypo/run-8jipcypo.wandb differ
